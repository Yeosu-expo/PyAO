Last module: norm
Model Len: 163
Per-module activation-GB to execution-time throughput (GB/s):
layers.5.mlp.down_proj, Linear: time=0.000021s, act=0.1250GB, throughput=6026.298851GB/s
layers.10.mlp.down_proj, Linear: time=0.000022s, act=0.1250GB, throughput=5761.406593GB/s
layers.12.mlp.down_proj, Linear: time=0.000022s, act=0.1250GB, throughput=5761.406593GB/s
layers.14.mlp.down_proj, Linear: time=0.000022s, act=0.1250GB, throughput=5761.406593GB/s
layers.6.mlp.down_proj, Linear: time=0.000022s, act=0.1250GB, throughput=5577.531915GB/s
layers.11.mlp.down_proj, Linear: time=0.000022s, act=0.1250GB, throughput=5577.531915GB/s
layers.3.mlp.down_proj, Linear: time=0.000023s, act=0.1250GB, throughput=5518.821053GB/s
layers.9.mlp.down_proj, Linear: time=0.000023s, act=0.1250GB, throughput=5461.333333GB/s
layers.13.mlp.down_proj, Linear: time=0.000023s, act=0.1250GB, throughput=5461.333333GB/s
layers.1.mlp.down_proj, Linear: time=0.000023s, act=0.1250GB, throughput=5405.030928GB/s
layers.2.mlp.down_proj, Linear: time=0.000023s, act=0.1250GB, throughput=5405.030928GB/s
layers.7.mlp.down_proj, Linear: time=0.000023s, act=0.1250GB, throughput=5405.030928GB/s
layers.8.mlp.down_proj, Linear: time=0.000024s, act=0.1250GB, throughput=5295.838384GB/s
layers.4.mlp.down_proj, Linear: time=0.000025s, act=0.1250GB, throughput=5090.174757GB/s
layers.15.mlp.down_proj, Linear: time=0.000025s, act=0.1250GB, throughput=5090.174757GB/s
layers.0.mlp.down_proj, Linear: time=0.000056s, act=0.1250GB, throughput=2221.559322GB/s
layers.10.self_attn.o_proj, Linear: time=0.000023s, act=0.0474GB, throughput=2091.116447GB/s
layers.4.self_attn.o_proj, Linear: time=0.000025s, act=0.0474GB, throughput=1928.699636GB/s
layers.6.self_attn.o_proj, Linear: time=0.000025s, act=0.0474GB, throughput=1928.699636GB/s
layers.8.self_attn.o_proj, Linear: time=0.000025s, act=0.0474GB, throughput=1928.699636GB/s
layers.2.self_attn.o_proj, Linear: time=0.000028s, act=0.0474GB, throughput=1697.915064GB/s
layers.14.self_attn.o_proj, Linear: time=0.000029s, act=0.0474GB, throughput=1641.785640GB/s
layers.12.self_attn.o_proj, Linear: time=0.000029s, act=0.0474GB, throughput=1628.328381GB/s
layers.0.self_attn.o_proj, Linear: time=0.000045s, act=0.0474GB, throughput=1056.681184GB/s
layers.14.self_attn.v_proj, Linear: time=0.000013s, act=0.0098GB, throughput=758.518519GB/s
layers.15.self_attn.v_proj, Linear: time=0.000013s, act=0.0098GB, throughput=758.518519GB/s
layers.13.self_attn.q_proj, Linear: time=0.000021s, act=0.0156GB, throughput=753.287356GB/s
layers.13.self_attn.v_proj, Linear: time=0.000013s, act=0.0098GB, throughput=744.727273GB/s
layers.6.self_attn.q_proj, Linear: time=0.000021s, act=0.0156GB, throughput=728.177778GB/s
layers.14.self_attn.q_proj, Linear: time=0.000022s, act=0.0156GB, throughput=712.347826GB/s
layers.11.self_attn.q_proj, Linear: time=0.000022s, act=0.0156GB, throughput=697.191489GB/s
layers.15.self_attn.q_proj, Linear: time=0.000022s, act=0.0156GB, throughput=697.191489GB/s
layers.3.self_attn.k_proj, Linear: time=0.000014s, act=0.0098GB, throughput=694.237288GB/s
layers.5.self_attn.v_proj, Linear: time=0.000014s, act=0.0098GB, throughput=694.237288GB/s
layers.9.self_attn.v_proj, Linear: time=0.000014s, act=0.0098GB, throughput=694.237288GB/s
layers.2.self_attn.q_proj, Linear: time=0.000023s, act=0.0156GB, throughput=689.852632GB/s
layers.12.self_attn.q_proj, Linear: time=0.000023s, act=0.0156GB, throughput=689.852632GB/s
layers.10.self_attn.q_proj, Linear: time=0.000023s, act=0.0156GB, throughput=682.666667GB/s
layers.11.self_attn.v_proj, Linear: time=0.000014s, act=0.0098GB, throughput=682.666667GB/s
layers.3.self_attn.q_proj, Linear: time=0.000024s, act=0.0156GB, throughput=661.979798GB/s
layers.4.self_attn.q_proj, Linear: time=0.000024s, act=0.0156GB, throughput=661.979798GB/s
layers.6.self_attn.k_proj, Linear: time=0.000015s, act=0.0098GB, throughput=660.645161GB/s
layers.6.self_attn.v_proj, Linear: time=0.000015s, act=0.0098GB, throughput=660.645161GB/s
layers.13.self_attn.k_proj, Linear: time=0.000015s, act=0.0098GB, throughput=660.645161GB/s
layers.6.mlp.up_proj, Linear: time=0.000060s, act=0.0391GB, throughput=655.360000GB/s
layers.8.self_attn.q_proj, Linear: time=0.000024s, act=0.0156GB, throughput=655.360000GB/s
layers.9.self_attn.o_proj, Linear: time=0.000072s, act=0.0474GB, throughput=653.473890GB/s
layers.1.self_attn.o_proj, Linear: time=0.000073s, act=0.0474GB, throughput=651.331352GB/s
layers.2.self_attn.v_proj, Linear: time=0.000015s, act=0.0098GB, throughput=650.158730GB/s
layers.7.self_attn.v_proj, Linear: time=0.000015s, act=0.0098GB, throughput=650.158730GB/s
layers.10.self_attn.k_proj, Linear: time=0.000015s, act=0.0098GB, throughput=650.158730GB/s
layers.11.self_attn.k_proj, Linear: time=0.000015s, act=0.0098GB, throughput=650.158730GB/s
layers.12.self_attn.v_proj, Linear: time=0.000015s, act=0.0098GB, throughput=650.158730GB/s
layers.11.mlp.act_fn, SiLU: time=0.000048s, act=0.0312GB, throughput=648.871287GB/s
layers.7.self_attn.q_proj, Linear: time=0.000024s, act=0.0156GB, throughput=642.509804GB/s
layers.8.mlp.act_fn, SiLU: time=0.000049s, act=0.0312GB, throughput=642.509804GB/s
layers.15.mlp.up_proj, Linear: time=0.000061s, act=0.0391GB, throughput=642.509804GB/s
layers.10.self_attn.v_proj, Linear: time=0.000015s, act=0.0098GB, throughput=640.000000GB/s
layers.7.self_attn.o_proj, Linear: time=0.000074s, act=0.0474GB, throughput=638.765474GB/s
layers.9.mlp.up_proj, Linear: time=0.000061s, act=0.0391GB, throughput=637.509728GB/s
layers.13.mlp.up_proj, Linear: time=0.000062s, act=0.0391GB, throughput=635.038760GB/s
layers.1.mlp.up_proj, Linear: time=0.000062s, act=0.0391GB, throughput=630.153846GB/s
layers.9.mlp.act_fn, SiLU: time=0.000050s, act=0.0312GB, throughput=630.153846GB/s
layers.15.self_attn.k_proj, Linear: time=0.000015s, act=0.0098GB, throughput=630.153846GB/s
layers.3.self_attn.o_proj, Linear: time=0.000075s, act=0.0474GB, throughput=628.658426GB/s
layers.6.mlp.act_fn, SiLU: time=0.000050s, act=0.0312GB, throughput=627.138756GB/s
layers.5.mlp.act_fn, SiLU: time=0.000050s, act=0.0312GB, throughput=624.152381GB/s
layers.14.mlp.act_fn, SiLU: time=0.000050s, act=0.0312GB, throughput=621.194313GB/s
layers.10.mlp.up_proj, Linear: time=0.000063s, act=0.0391GB, throughput=620.606061GB/s
layers.9.self_attn.q_proj, Linear: time=0.000025s, act=0.0156GB, throughput=618.264151GB/s
layers.15.self_attn.o_proj, Linear: time=0.000077s, act=0.0474GB, throughput=616.944293GB/s
layers.5.self_attn.q_proj, Linear: time=0.000026s, act=0.0156GB, throughput=612.485981GB/s
layers.7.self_attn.k_proj, Linear: time=0.000016s, act=0.0098GB, throughput=611.343284GB/s
layers.14.self_attn.k_proj, Linear: time=0.000016s, act=0.0098GB, throughput=611.343284GB/s
layers.7.mlp.up_proj, Linear: time=0.000064s, act=0.0391GB, throughput=609.070632GB/s
layers.3.mlp.act_fn, SiLU: time=0.000051s, act=0.0312GB, throughput=606.814815GB/s
layers.4.mlp.act_fn, SiLU: time=0.000051s, act=0.0312GB, throughput=606.814815GB/s
layers.2.mlp.up_proj, Linear: time=0.000065s, act=0.0391GB, throughput=602.352941GB/s
layers.8.self_attn.k_proj, Linear: time=0.000016s, act=0.0098GB, throughput=602.352941GB/s
layers.12.mlp.up_proj, Linear: time=0.000065s, act=0.0391GB, throughput=602.352941GB/s
layers.14.mlp.up_proj, Linear: time=0.000065s, act=0.0391GB, throughput=600.146520GB/s
layers.13.self_attn.o_proj, Linear: time=0.000079s, act=0.0474GB, throughput=598.361634GB/s
layers.5.self_attn.o_proj, Linear: time=0.000079s, act=0.0474GB, throughput=596.564752GB/s
layers.5.self_attn.k_proj, Linear: time=0.000016s, act=0.0098GB, throughput=593.623188GB/s
layers.2.mlp.act_fn, SiLU: time=0.000053s, act=0.0312GB, throughput=593.085973GB/s
layers.11.mlp.up_proj, Linear: time=0.000066s, act=0.0391GB, throughput=591.480144GB/s
layers.10.mlp.act_fn, SiLU: time=0.000053s, act=0.0312GB, throughput=590.414414GB/s
layers.12.mlp.act_fn, SiLU: time=0.000053s, act=0.0312GB, throughput=590.414414GB/s
layers.11.self_attn.o_proj, Linear: time=0.000081s, act=0.0474GB, throughput=586.006084GB/s
layers.6.mlp.gate_proj, Linear: time=0.000067s, act=0.0391GB, throughput=585.142857GB/s
layers.4.mlp.up_proj, Linear: time=0.000067s, act=0.0391GB, throughput=580.992908GB/s
layers.8.mlp.gate_proj, Linear: time=0.000067s, act=0.0391GB, throughput=578.939929GB/s
layers.2.self_attn.k_proj, Linear: time=0.000017s, act=0.0098GB, throughput=576.901408GB/s
layers.4.self_attn.v_proj, Linear: time=0.000017s, act=0.0098GB, throughput=576.901408GB/s
layers.8.mlp.up_proj, Linear: time=0.000068s, act=0.0391GB, throughput=576.901408GB/s
layers.9.self_attn.k_proj, Linear: time=0.000017s, act=0.0098GB, throughput=576.901408GB/s
layers.5.mlp.gate_proj, Linear: time=0.000068s, act=0.0391GB, throughput=574.877193GB/s
layers.3.mlp.up_proj, Linear: time=0.000069s, act=0.0391GB, throughput=566.920415GB/s
layers.3.mlp.gate_proj, Linear: time=0.000069s, act=0.0391GB, throughput=563.024055GB/s
layers.7.mlp.gate_proj, Linear: time=0.000069s, act=0.0391GB, throughput=563.024055GB/s
layers.15.mlp.gate_proj, Linear: time=0.000070s, act=0.0391GB, throughput=561.095890GB/s
layers.15.mlp.act_fn, SiLU: time=0.000056s, act=0.0312GB, throughput=557.753191GB/s
layers.1.mlp.gate_proj, Linear: time=0.000070s, act=0.0391GB, throughput=557.278912GB/s
layers.5.mlp.up_proj, Linear: time=0.000070s, act=0.0391GB, throughput=555.389831GB/s
layers.14.mlp.gate_proj, Linear: time=0.000070s, act=0.0391GB, throughput=555.389831GB/s
layers.4.self_attn.k_proj, Linear: time=0.000018s, act=0.0098GB, throughput=553.513514GB/s
layers.12.self_attn.k_proj, Linear: time=0.000018s, act=0.0098GB, throughput=553.513514GB/s
layers.13.mlp.gate_proj, Linear: time=0.000071s, act=0.0391GB, throughput=549.798658GB/s
layers.1.mlp.act_fn, SiLU: time=0.000057s, act=0.0312GB, throughput=543.867220GB/s
layers.1.self_attn.v_proj, Linear: time=0.000018s, act=0.0098GB, throughput=531.948052GB/s
layers.4.mlp.gate_proj, Linear: time=0.000073s, act=0.0391GB, throughput=531.948052GB/s
layers.13.mlp.act_fn, SiLU: time=0.000062s, act=0.0312GB, throughput=500.274809GB/s
layers.8.self_attn.v_proj, Linear: time=0.000020s, act=0.0098GB, throughput=499.512195GB/s
layers.11.mlp.gate_proj, Linear: time=0.000079s, act=0.0391GB, throughput=494.984894GB/s
layers.1.self_attn.k_proj, Linear: time=0.000020s, act=0.0098GB, throughput=481.882353GB/s
layers.1.self_attn.q_proj, Linear: time=0.000036s, act=0.0156GB, throughput=439.838926GB/s
layers.12.mlp.gate_proj, Linear: time=0.000090s, act=0.0391GB, throughput=434.588859GB/s
layers.9.mlp.gate_proj, Linear: time=0.000091s, act=0.0391GB, throughput=427.780679GB/s
layers.10.mlp.gate_proj, Linear: time=0.000096s, act=0.0391GB, throughput=405.544554GB/s
layers.2.mlp.gate_proj, Linear: time=0.000097s, act=0.0391GB, throughput=402.555283GB/s
layers.0.mlp.up_proj, Linear: time=0.000104s, act=0.0391GB, throughput=374.919908GB/s
layers.7.mlp.act_fn, SiLU: time=0.000086s, act=0.0312GB, throughput=362.077348GB/s
layers.0.mlp.gate_proj, Linear: time=0.000117s, act=0.0391GB, throughput=333.008130GB/s
layers.0.self_attn.v_proj, Linear: time=0.000031s, act=0.0098GB, throughput=315.076923GB/s
layers.13.input_layernorm, LlamaRMSNorm: time=0.000142s, act=0.0391GB, throughput=275.495798GB/s
layers.2.input_layernorm, LlamaRMSNorm: time=0.000143s, act=0.0391GB, throughput=273.656093GB/s
layers.6.input_layernorm, LlamaRMSNorm: time=0.000143s, act=0.0391GB, throughput=273.656093GB/s
norm, LlamaRMSNorm: time=0.000145s, act=0.0391GB, throughput=270.049423GB/s
layers.9.post_attention_layernorm, LlamaRMSNorm: time=0.000145s, act=0.0391GB, throughput=269.605263GB/s
layers.8.input_layernorm, LlamaRMSNorm: time=0.000146s, act=0.0391GB, throughput=268.281506GB/s
layers.11.input_layernorm, LlamaRMSNorm: time=0.000148s, act=0.0391GB, throughput=264.814216GB/s
layers.9.input_layernorm, LlamaRMSNorm: time=0.000150s, act=0.0391GB, throughput=260.190476GB/s
layers.5.input_layernorm, LlamaRMSNorm: time=0.000153s, act=0.0391GB, throughput=255.725429GB/s
layers.14.input_layernorm, LlamaRMSNorm: time=0.000154s, act=0.0391GB, throughput=253.746130GB/s
layers.15.post_attention_layernorm, LlamaRMSNorm: time=0.000155s, act=0.0391GB, throughput=252.184615GB/s
layers.7.post_attention_layernorm, LlamaRMSNorm: time=0.000155s, act=0.0391GB, throughput=251.797235GB/s
layers.13.post_attention_layernorm, LlamaRMSNorm: time=0.000159s, act=0.0391GB, throughput=246.496241GB/s
layers.3.post_attention_layernorm, LlamaRMSNorm: time=0.000167s, act=0.0391GB, throughput=234.171429GB/s
layers.15.input_layernorm, LlamaRMSNorm: time=0.000168s, act=0.0391GB, throughput=232.840909GB/s
layers.10.input_layernorm, LlamaRMSNorm: time=0.000170s, act=0.0391GB, throughput=229.901823GB/s
layers.11.post_attention_layernorm, LlamaRMSNorm: time=0.000172s, act=0.0391GB, throughput=227.350902GB/s
layers.3.self_attn.v_proj, Linear: time=0.000044s, act=0.0098GB, throughput=223.825137GB/s
layers.7.input_layernorm, LlamaRMSNorm: time=0.000178s, act=0.0391GB, throughput=220.026846GB/s
layers.3.input_layernorm, LlamaRMSNorm: time=0.000179s, act=0.0391GB, throughput=218.560000GB/s
layers.12.input_layernorm, LlamaRMSNorm: time=0.000182s, act=0.0391GB, throughput=214.554974GB/s
layers.5.post_attention_layernorm, LlamaRMSNorm: time=0.000187s, act=0.0391GB, throughput=208.815287GB/s
layers.12.post_attention_layernorm, LlamaRMSNorm: time=0.000190s, act=0.0391GB, throughput=205.413534GB/s
layers.0.post_attention_layernorm, LlamaRMSNorm: time=0.000191s, act=0.0391GB, throughput=204.900000GB/s
layers.10.post_attention_layernorm, LlamaRMSNorm: time=0.000192s, act=0.0391GB, throughput=203.880597GB/s
layers.6.post_attention_layernorm, LlamaRMSNorm: time=0.000197s, act=0.0391GB, throughput=198.690909GB/s
layers.1.post_attention_layernorm, LlamaRMSNorm: time=0.000197s, act=0.0391GB, throughput=197.971014GB/s
layers.14.post_attention_layernorm, LlamaRMSNorm: time=0.000198s, act=0.0391GB, throughput=197.256318GB/s
layers.4.input_layernorm, LlamaRMSNorm: time=0.000200s, act=0.0391GB, throughput=195.842294GB/s
layers.4.post_attention_layernorm, LlamaRMSNorm: time=0.000200s, act=0.0391GB, throughput=195.608592GB/s
layers.2.post_attention_layernorm, LlamaRMSNorm: time=0.000205s, act=0.0391GB, throughput=190.383275GB/s
layers.8.post_attention_layernorm, LlamaRMSNorm: time=0.000223s, act=0.0391GB, throughput=175.503212GB/s
layers.1.input_layernorm, LlamaRMSNorm: time=0.000233s, act=0.0391GB, throughput=167.950820GB/s
layers.0.self_attn.k_proj, Linear: time=0.000096s, act=0.0098GB, throughput=102.144638GB/s
layers.0.mlp.act_fn, SiLU: time=0.004264s, act=0.0312GB, throughput=7.329419GB/s
layers.0.input_layernorm, LlamaRMSNorm: time=0.028765s, act=0.0391GB, throughput=1.358641GB/s
layers.0.self_attn.q_proj, Linear: time=0.074446s, act=0.0156GB, throughput=0.209882GB/s
embed_tokens, Embedding: time=0.038659s, act=0.0000GB, throughput=0.000395GB/s
rotary_emb, LlamaRotaryEmbedding: time=0.056460s, act=0.0000GB, throughput=0.000000GB/s
None, None: time=0.000000s, act=2.4541GB, throughput=0.000000GB/s

Outlier modules (IQR method):
layers.0.self_attn.o_proj: throughput=1056.681184GB/s (upper bound: 927.727247GB/s)
layers.0.mlp.down_proj: throughput=2221.559322GB/s (upper bound: 927.727247GB/s)
layers.1.mlp.down_proj: throughput=5405.030928GB/s (upper bound: 927.727247GB/s)
layers.2.self_attn.o_proj: throughput=1697.915064GB/s (upper bound: 927.727247GB/s)
layers.2.mlp.down_proj: throughput=5405.030928GB/s (upper bound: 927.727247GB/s)
layers.3.mlp.down_proj: throughput=5518.821053GB/s (upper bound: 927.727247GB/s)
layers.4.self_attn.o_proj: throughput=1928.699636GB/s (upper bound: 927.727247GB/s)
layers.4.mlp.down_proj: throughput=5090.174757GB/s (upper bound: 927.727247GB/s)
layers.5.mlp.down_proj: throughput=6026.298851GB/s (upper bound: 927.727247GB/s)
layers.6.self_attn.o_proj: throughput=1928.699636GB/s (upper bound: 927.727247GB/s)
layers.6.mlp.down_proj: throughput=5577.531915GB/s (upper bound: 927.727247GB/s)
layers.7.mlp.down_proj: throughput=5405.030928GB/s (upper bound: 927.727247GB/s)
layers.8.self_attn.o_proj: throughput=1928.699636GB/s (upper bound: 927.727247GB/s)
layers.8.mlp.down_proj: throughput=5295.838384GB/s (upper bound: 927.727247GB/s)
layers.9.mlp.down_proj: throughput=5461.333333GB/s (upper bound: 927.727247GB/s)
layers.10.self_attn.o_proj: throughput=2091.116447GB/s (upper bound: 927.727247GB/s)
layers.10.mlp.down_proj: throughput=5761.406593GB/s (upper bound: 927.727247GB/s)
layers.11.mlp.down_proj: throughput=5577.531915GB/s (upper bound: 927.727247GB/s)
layers.12.self_attn.o_proj: throughput=1628.328381GB/s (upper bound: 927.727247GB/s)
layers.12.mlp.down_proj: throughput=5761.406593GB/s (upper bound: 927.727247GB/s)
layers.13.mlp.down_proj: throughput=5461.333333GB/s (upper bound: 927.727247GB/s)
layers.14.self_attn.o_proj: throughput=1641.785640GB/s (upper bound: 927.727247GB/s)
layers.14.mlp.down_proj: throughput=5761.406593GB/s (upper bound: 927.727247GB/s)
layers.15.mlp.down_proj: throughput=5090.174757GB/s (upper bound: 927.727247GB/s)

Outlier count: 24 / 163 layers (14.72%)
